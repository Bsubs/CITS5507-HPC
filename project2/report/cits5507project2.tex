% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={CITS7707 Project 2 Report},
  pdfauthor={Joo Kai Tay (22489437)},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{CITS7707 Project 2 Report}
\author{Joo Kai Tay (22489437)}
\date{2023-10-16}

\begin{document}
\maketitle

\hypertarget{project-overview}{%
\section{Project Overview}\label{project-overview}}

Fish School Behaviour (FSB) is a heuristic algorithm used for
multi-dimensional optimization problems. This project aims to test
various sequential and parallel implementations of search based on the
Fish School Behavior (FSB) algorithm using MPI(Message Passing
Interface) to allow the program to execute on varying number of nodes.
The performance for each experiment will be measured by the time taken
to execute the program given a number of fish and steps.

\hypertarget{fish-simulation}{%
\section{Fish simulation}\label{fish-simulation}}

\hypertarget{fish-structure}{%
\subsection{Fish Structure}\label{fish-structure}}

The file fish.h contains the definition of a fish in this simulation.
The struct Fish contains the following parameters:

\begin{itemize}
\tightlist
\item
  \textbf{double euclDist}: This is the euclidean distance of the fish
  from the origin in the previous step
\item
  \textbf{double x\_c}: This holds the x coordinate of the fish in the
  current step
\item
  \textbf{double y\_c}: This holds the y coordinate of the fish in the
  current step
\item
  \textbf{double weight\_c}: This holds the weight of the fish in the
  current step
\item
  \textbf{double weight\_p}: This holds the weight of the fish in the
  previous step
\end{itemize}

\begin{verbatim}
typedef struct fish {
    double euclDist;
    double x_c;
    double y_c;
    double weight_c;
    double weight_p;
} Fish;
\end{verbatim}

In order to simplify passing the Fish structure using MPI, we will
create a custom MPI datatype named \texttt{MPI\_FISH}. This datatype
will have the same fields as the fish structure described above.

\begin{verbatim}
MPI_Datatype create_mpi_fish_datatype() {
    MPI_Datatype MPI_FISH;
    MPI_Datatype types[NUMFIELDS] = {MPI_DOUBLE, MPI_DOUBLE, MPI_DOUBLE, MPI_DOUBLE, MPI_DOUBLE};
    int blocklengths[NUMFIELDS] = {1, 1, 1, 1, 1};
    MPI_Aint offsets[NUMFIELDS];

    offsets[0] = offsetof(Fish, euclDist);
    offsets[1] = offsetof(Fish, x_c);
    offsets[2] = offsetof(Fish, y_c);
    offsets[3] = offsetof(Fish, weight_c);
    offsets[4] = offsetof(Fish, weight_p);

    MPI_Type_create_struct(NUMFIELDS, blocklengths, offsets, types, &MPI_FISH);
    MPI_Type_commit(&MPI_FISH);
    return(MPI_FISH);
}
\end{verbatim}

\hypertarget{fish-methods}{%
\subsection{Fish methods}\label{fish-methods}}

fish.h also contains declarations for functions that are implemented in
fish.c

This function dynamically allocates memory in the heap for an array of
Fish structures and initializes each fish with random coordinates within
a 200x200 square which represents the pond. The parameter
\texttt{numfish} is used to specify the number of fish instead of a
\texttt{\#define\ NUMFISH} C preprocessor constant to allow for
experiments with different numbers of fish.

\begin{verbatim}
Fish* initializeFish(int numfish)
\end{verbatim}

This function updates the weight of a fish during a simulation step. It
takes 3 parameters:

\begin{itemize}
\tightlist
\item
  \textbf{Fish* fish1}: A pointer to the fish that will be updated
\item
  \textbf{double maxObj}: This is the maximum difference in distance
  between the previous step and the current step
\item
  \textbf{int step}: The current step of the function
\end{itemize}

\begin{verbatim}
void eat(Fish* fish1, double maxObj, int step)
\end{verbatim}

This function updates the position of a fish instance if its current
weight is less than two times the STARTWEIGHT. The new position is
calculated randomly within -0.1 and 0.1 and is then clamped to ensure it
remains within a 200 x 200 square.

\begin{verbatim}
void swim(Fish* fish1)
\end{verbatim}

\hypertarget{file-writing}{%
\section{File Writing}\label{file-writing}}

The deliverable is investigating the method to deliver effective
communication between MPI processes. We want to establish the best
method for communication so that the code in the following sections will
run as efficiently as possible. The master process (rank == 0) generates
the fish data and distributes it to the 4 nodes (including itself) using
gather. Immediately after, it will write to a file named
\texttt{initial\_data.txt}. The master process will then gather the data
back and write it into \texttt{final\_data.txt}.

In this experiment, we will compare collective communication operations
(like MPI\_Scatter and MPI\_Gather) with point-to-point operations (like
MPI\_Send and MPI\_Recv) to determine which is the most suitable for
transmitting the FISH data.

\begin{itemize}
\tightlist
\item
  Source file: \texttt{experiment0.c} \& \texttt{experiment1.c}
\item
  Run with: \texttt{sbatch\ experiment0.sh} \&
  \texttt{sbatch\ experiment1.sh}
\item
  Number of fish: 100,000,000
\item
  Number of nodes: 4
\end{itemize}

The file experiment0.c uses \texttt{MPI\_Scatter\ and\ MPI\_Gather} to
distribute the Fish data to the 4 processes including the master and
gather to data back.

The file experiment1.c uses \texttt{MPI\_Send\ and\ MPI\_Receive} to
distribute the Fish data to the 4 processes including the master and
gather to data back.

\includegraphics{cits5507project2_files/figure-latex/unnamed-chunk-3-1.pdf}

From the plot above, we can see that using
\texttt{MPI\_Scatter\ \&\ MPI\_Gather} performed better than
\texttt{MPI\_Send\ and\ MPI\_Receive}. There could be several reasons
for this:

\begin{itemize}
\item
  Synchronization: Collective communication functions provide implicit
  synchronization. This means that processes participating in a
  collective operation will reach a known state relative to one another,
  which can be particularly useful when distributing data as you often
  want all processes to start computing on their respective data chunks
  simultaneously.
\item
  Less Overhead: Using collective communication reduces the overhead of
  initiating and managing multiple point-to-point communication calls.
  When distributing a dataset, we have to send data to multiple
  processes, and managing these communications individually can be
  inefficient.
\end{itemize}

\hypertarget{comparing-the-file-writing-outputs}{%
\subsection{Comparing the file writing
outputs}\label{comparing-the-file-writing-outputs}}

Afterwards to compare the outputs:

\begin{verbatim}
diff -s initial_data.txt final_data.txt
\end{verbatim}

The output of this command should be:
\texttt{Files\ initial\_data.txt\ and\ final\_data.txt\ are\ identical}

\hypertarget{experiment-methodology}{%
\section{Experiment Methodology}\label{experiment-methodology}}

The experiments will be compared against the best parallel and best
sequential functions as found in project 1.

\hypertarget{base-case---sequential}{%
\subsection{Base Case - Sequential}\label{base-case---sequential}}

The base case for this simulation was implemented using the
\texttt{void\ sequential(Fish*\ fishArray,\ int\ numfish,\ int\ numsteps)}
function implemented in \texttt{sequential.c}. The FSB problem was
tackled as such:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The outer loop runs for a number of iterations equal to
  \texttt{NUMSTEPS}.
\item
  The first of the inner loops iterates over all the fish present and
  finds the maximum difference in the position of the fish in the
  current and the fish in the previous round.
\item
  The second of the inner loops uses the maximum difference found in
  step 2 and performs the eat and swim operations on each fish in the
  array.
\item
  The final of the inner loops calculates the numerator and denominator
  variables for the barycentre.
\item
  The barycentre is calculated using the values from step 4.
\end{enumerate}

To run this simulation use the following command:

\begin{verbatim}
sbatch experiment3.sh
\end{verbatim}

\hypertarget{base-case---parallel}{%
\subsection{Base Case - Parallel}\label{base-case---parallel}}

The base case for parallel functions in this simulation is the
\texttt{void\ parallelReduction(Fish*\ fishArray,\ int\ numfish,\ int\ numsteps)}
function implemented in \texttt{parallel\_functions.c}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The first inner loop is parallelized using
  \texttt{\#pragma\ omp\ for\ reduction(max:maxDiff)} inside the
  parallel region. The reduction clause ensures that the
  \texttt{maxDiff} variable is updated in a thread-safe manner, to avoid
  any race conditions when updating the variable.
\item
  The second inner loop is parallelized using
  \texttt{\#pragma\ omp\ for} as the eat and swim operations are
  thread-safe, therefore, no special clauses need to be attached to this
  loop.
\item
  The last inner loop is also parallelized using
  \texttt{\#pragma\ omp\ for\ reduction(+:sumOfProduct,sumOfDistance)}
  to avoid race conditions when updating the two variables.
\end{enumerate}

To run this simulation use the following command:

\begin{verbatim}
sbatch experiment4.sh
\end{verbatim}

\hypertarget{experiments}{%
\section{Experiments}\label{experiments}}

\hypertarget{using-mpi-only}{%
\subsection{Using MPI Only}\label{using-mpi-only}}

In our initial experiment, we sought to evaluate the effect of using the
MPI framework on the performance of the FSB algorithm. This required
measuring the time of the MPI-only implementation, without any
involvement of OpenMP threads. We will be comparing the MPI
implementation with the base sequential implementation described above.

\begin{itemize}
\tightlist
\item
  Source file: \texttt{experiment3.c} \& \texttt{experiment5.c}
\item
  Run with: \texttt{sbatch\ experiment3.sh} \&
  \texttt{sbatch\ experiment5.sh}
\item
  Number of steps: 2000
\item
  Number of fish: 1,000,000
\item
  Number of nodes: 1, 2, 3, 4
\end{itemize}

\includegraphics{cits5507project2_files/figure-latex/unnamed-chunk-4-1.pdf}

The plot above shows the execution time as the number of MPI nodes
increase. The sequential baseline took 327.68 seconds to execute. At 2
nodes, the MPI process took 175.07 seconds to execute which is a 46.6\%
reduction in time taken. At 3 nodes, the MPI code took 116.31 seconds to
execute which is a 64.5\% reduction in the time taken. At 4 nodes, the
MPI code took 88.85 seconds to execute which is a 72.9\% reduction in
the time taken.

These percentages are very close to 0.5, 0.33 and 0.25 of the total time
taken. There could be 2 possible reasons why they did not hit these
numbers:

\begin{itemize}
\item
  The overheads from inter-process communication and synchronization
  caused the delay.
\item
  Amdahl's Law, which states that the speedup of a parallel program is
  limited by its sequential portion. In this case, as we increase the
  number of nodes, the sequential portion becomes a smaller fraction of
  the total execution time, leading to diminishing returns in speedup.
\end{itemize}

\hypertarget{comparing-different-communication-functions}{%
\subsection{Comparing different communication
functions}\label{comparing-different-communication-functions}}

\begin{itemize}
\tightlist
\item
  Source file: \texttt{experiment5.c} \& \texttt{experiment8.c}
\item
  Run with: \texttt{sbatch\ experiment5.sh} \&
  \texttt{sbatch\ experiment8.sh}
\item
  Number of steps: 2000
\item
  Number of fish: 1,000,000
\item
  Number of nodes: 2, 3, 4
\end{itemize}

The second experiment in MPI will be a follow up of the file writing
experiment where we experiment with \texttt{MPI\_Scatter} \&
\texttt{MPI\_Gather} and \texttt{MPI\_Send} \& \texttt{MPI\_Receive}.
The earlier experiment tested the two different communication methods
with 4 MPI nodes, and in this experiment we will vary the number of
nodes to observe if changing the number of MPI nodes will affect the
performance of the communication methods.

\includegraphics{cits5507project2_files/figure-latex/unnamed-chunk-5-1.pdf}

As seen from the plot above, using \texttt{MPI\_Scatter} \&
\texttt{MPI\_Gather} is more efficient than \texttt{MPI\_Send} \&
\texttt{MPI\_Receive} over all numbers of nodes. This is to be expected
as collective communication functions, like MPI\_Scatter and
MPI\_Gather, are often implemented using algorithms that are optimized
for specific network topologies and hardware. They can take advantage of
knowledge about the entire communication pattern to optimize data
movement.

Collective communications are also more predictable since they involve a
known set of processes. This predictability can be exploited by the
underlying MPI implementation to further optimize communication.

Based on this knowledge, we will be using \texttt{MPI\_Scatter} \&
\texttt{MPI\_Gather} for communication between processes where we have
to distribute fish in subsequent experiments.

\hypertarget{comparing-mpi-sequential-and-mpi-parallel}{%
\subsection{Comparing MPI Sequential and MPI
Parallel}\label{comparing-mpi-sequential-and-mpi-parallel}}

\begin{itemize}
\tightlist
\item
  Source file: \texttt{experiment3.c}, \texttt{experiment5.c},
  \texttt{experiment6.c}
\item
  Run with: \texttt{sbatch\ experiment3.sh},
  \texttt{sbatch\ experiment5.sh}, \texttt{sbatch\ experiment6.sh}
\item
  Number of steps: 2000
\item
  Number of fish: 1,000,000
\item
  Number of nodes: 2, 3, 4
\end{itemize}

In this experiment, we will be comparing three scenarios:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  MPI Only: This implementation makes use of just MPI with no threads.
\item
  MPI Parallel: This implementation makes use of MPI and OpenMP but uses
  MPI\_THREAD\_SINGLE (MPI\_Init)
\item
  MPI Parallel Threads: This implementation makes use of MPI and OpenMP
  and uses MPI\_Thread\_FUNNELED
\end{enumerate}

In theory, \texttt{MPI\_Parallel} is still thread safe as it does not
make MPI calls from anywhere other than the master thread, and the only
difference between is using \texttt{MPI\_Init} and
\texttt{MPI\_Init\_thread}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(df,  }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ NumNodes)) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{y=}\NormalTok{MPISeq)) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{y=}\NormalTok{MPIParBaseNodes)) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{y=}\NormalTok{MPIThreadSafe)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ MPISeq, }\AttributeTok{color =} \StringTok{"MPI Only"}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ MPIParBaseNodes, }\AttributeTok{color =} \StringTok{"MPI Parallel"}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ MPIThreadSafe, }\AttributeTok{color =} \StringTok{"MPI Parallel Threads"}\NormalTok{)) }\SpecialCharTok{+}
  
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Execution Time vs Number of MPI Nodes \& OpenMP Threads"}\NormalTok{,}
    \AttributeTok{x =} \StringTok{"Number of MPI Nodes"}\NormalTok{,}
    \AttributeTok{y =} \StringTok{"Time (s)"}\NormalTok{,}
    \AttributeTok{color =} \StringTok{"Number of Threads"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{cits5507project2_files/figure-latex/unnamed-chunk-6-1.pdf}

The plot above shows the execution time for the three implementations.
It can be observed that the time differential between them is very
small. In order to provide a better understanding, a bar chart of the
times taken at 4 nodes is plotted below.

\includegraphics{cits5507project2_files/figure-latex/unnamed-chunk-7-1.pdf}

The plot above gives us a more detailed look at the timing of the three
implementations. As previously stated, they are all very close in time
taken. This is different from the results from project 1 where the
implementations that made use of OpenMP were significantly slower than
their sequential counterpart.

\hypertarget{mpi-parallel-with-different-threads}{%
\subsection{MPI parallel with different
threads}\label{mpi-parallel-with-different-threads}}

\begin{itemize}
\tightlist
\item
  Source file: \texttt{experiment2.c}, \texttt{experiment5.c}
\item
  Run with: \texttt{sbatch\ experiment3.sh},
  \texttt{sbatch\ experiment5.sh}
\item
  Number of steps: 2000
\item
  Number of fish: 1,000,000
\item
  Number of nodes: 4
\item
  Number of threads: 2, 4, 8, 16
\end{itemize}

In this experiment, we will investigate the increase in time as the
number of threads increase and compare it to the base parallel
implementation.

\includegraphics{cits5507project2_files/figure-latex/unnamed-chunk-8-1.pdf}

The plot above shows the increase in time taken as the number of threads
increase for MPI + OpenMP and just OpenMP. For just OpenMP, the increase
in time as the number of threads increase is relatively linear. However
for MPI + OpenMP, the increase from 2-4 threads is significantly smaller
than the increase from 4-8 threads.

\hypertarget{compare-all-functions}{%
\subsection{Compare all functions}\label{compare-all-functions}}

\includegraphics{cits5507project2_files/figure-latex/unnamed-chunk-9-1.pdf}

The plot above shows the comparison of the functions with MPI and
without MPI. It can be observed that as the size of the input grows, the
benefits of using MPI increases. However, as seen in the results from
both project 1 and project 2, the benefits of using multiple threads per
process does not seem to benefit the Fish School Behavior (FSB)
algorithm, as increasing the number of threads degrades the performance
even for large input sizes (100,000,000 fish). Therefore, if we were to
be implementing this algorithm in a research environment, we would focus
on increasing the amount of resources for MPI, such as the ability to
request more nodes. This would allow us to get the best possible
performance.

\end{document}
